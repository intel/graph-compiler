################################################################################
# Copyright 2024 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

# Use class field
# do not use the dict generated by json package directly

from typing import List, Dict, Any
import gc_mlir.ir
from gc_mlir._mlir_libs._mlir.ir import (
    BoolAttr,
    FloatAttr,
    IntegerAttr,
    DenseI64ArrayAttr,
    DenseF32ArrayAttr,
    StringAttr,
)
from functools import reduce
import operator


def check_type(field: str, expected_type: type, value_type: type):
    if expected_type != value_type:
        raise Exception(
            "field %s expect type: %s, not %s" % (field, expected_type, value_type)
        )


def set_required_field(
    obj: Any, d: Dict[str, Any], field: str, expected_type: type | None = None
):
    if field not in d:
        raise Exception("field %s is a required field and not provided" % field)
    if expected_type is not None:
        check_type(field, expected_type, type(d[field]))
    obj.__setattr__(field, d[field])


class LogicalTensor:
    id: int
    dtype: str
    shape: List[int]
    stride: List[int]
    layout_type: str
    property_type: str

    def __init__(self, lt: Dict[str, Any]) -> None:
        check_type("logical tensor", dict, type(lt))
        set_required_field(self, lt, "id", int)
        set_required_field(self, lt, "dtype", str)
        set_required_field(self, lt, "shape", list)
        set_required_field(self, lt, "stride", list)
        set_required_field(self, lt, "layout_type", str)
        set_required_field(self, lt, "property_type", str)

    def nelem(self) -> int:
        return reduce(operator.mul, self.shape)


class Attribute:
    type: str
    value: Any

    def __init__(self, attr: Dict[str, Any]) -> None:
        set_required_field(self, attr, "type", str)
        set_required_field(self, attr, "value")
        if self.type == "bool":
            # translate 0/1 to False/True
            self.value = bool(self.value)


class Op:
    id: int
    name: str
    kind: str
    attrs: Dict[str, Attribute]
    inputs: List[LogicalTensor]
    outputs: List[LogicalTensor]

    def __init__(self, op: Dict[str, Any]) -> None:
        check_type("json op", dict, type(op))
        set_required_field(self, op, "id", int)
        set_required_field(self, op, "name", str)
        set_required_field(self, op, "kind", str)

        check_type("inputs", list, type(op["inputs"]))
        check_type("outputs", list, type(op["outputs"]))
        check_type("attrs", dict, type(op["attrs"]))

        self.inputs = []
        for lt in op["inputs"]:
            self.inputs.append(LogicalTensor(lt))
        self.outputs = []
        for lt in op["outputs"]:
            self.outputs.append(LogicalTensor(lt))
        self.attrs = {}
        for attr in op["attrs"]:
            self.attrs[attr] = Attribute(op["attrs"][attr])

    def get_required_attr(self, name: str, expect_type: type) -> Any:
        if name not in self.attrs:
            raise Exception("attr %s is required but not set" % name)
        elif expect_type == float and type(self.attrs[name].value) == int:
            return float(self.attrs[name].value)
        elif type(self.attrs[name].value) != expect_type:
            raise Exception(
                "attr %s is expect type %s, not %s"
                % (name, expect_type, type(self.attrs[name].value))
            )
        return self.attrs[name].value

    def get_optional_attr(
        self, name: str, expect_type: type, default_value: Any
    ) -> Any:
        if name not in self.attrs:
            return default_value
        elif expect_type == float and type(self.attrs[name].value) == int:
            return float(self.attrs[name].value)
        elif type(self.attrs[name].value) != expect_type:
            raise Exception(
                "attr %s is expect type %s, not %s"
                % (name, expect_type, type(self.attrs[name].value))
            )
        return self.attrs[name].value


from .ops import dialect_op_kind


class Graph:
    # not using now, just a placeholder
    version: str
    engine_kind: str
    fpmath_mode: str

    graph: List[Op]

    def __init__(self, g: Any) -> None:
        check_type("json graph", dict, type(g))
        set_required_field(self, g, "version", str)
        set_required_field(self, g, "engine_kind", str)
        set_required_field(self, g, "fpmath_mode", str)

        check_type("graph", list, type(g["graph"]))
        self.graph = []
        for op in g["graph"]:
            self.graph.append(Op(op))


class MLIRGraph:
    # mlir variable -> tensor id
    var_to_id: Dict[str, int]
    module: gc_mlir.ir.Module
    graph_object: Any

    def __init__(self, module: gc_mlir.ir.Module):
        self.var_to_id: Dict[str, int] = {}
        self.graph_object = {
            "version": "3.0.0",
            "engine_kind": "cpu",
            "fpmath_mode": "strict",
            "graph": [],
        }
        self.module = module

    def convert_to_json(self, entry: str) -> Any:
        # module is at "builtin.module"
        # try to find entry op first
        entry_op: gc_mlir.ir.OpView | None = None
        for op in self.module.operation.opview.regions[0].blocks[0].operations:
            if str(op.name) == entry:
                entry_op = op
                break
        if entry_op is None:
            raise Exception("entry function %s is not found at the top level" % entry)
        else:
            self.dfs_op(entry_op)
            return self.graph_object

    def gen_stride(self, shape: List[int]) -> List[int]:
        stride: List[int] = [1] * len(shape)
        for i in range(1, len(shape)):
            stride[-i - 1] = stride[-i] * shape[-i]
        return stride

    def dfs_op(self, op: gc_mlir.ir.OpView):
        dialect_call: str = str(op.name)
        if dialect_call.startswith("onednn_graph"):
            dialect_op: str = dialect_call.split(".")[1]

            graph_op: Any = {
                "kind": dialect_op_kind[dialect_op],
                "name": dialect_call,
                "attrs": {},
                "inputs": [],
                "outputs": [],
            }
            for attribute in op.attributes:
                if isinstance(attribute.attr, BoolAttr):
                    graph_op["attrs"][attribute.name] = {
                        "type": "bool",
                        "value": 1 if attribute.attr.__bool__() else 0,
                    }
                elif isinstance(attribute.attr, FloatAttr):
                    graph_op["attrs"][attribute.name] = {
                        "type": "f32",
                        "value": attribute.attr.__float__(),
                    }
                elif isinstance(attribute.attr, IntegerAttr):
                    graph_op["attrs"][attribute.name] = {
                        "type": "s64",
                        "value": attribute.attr.__int__(),
                    }
                elif isinstance(attribute.attr, DenseI64ArrayAttr):
                    i64_array: List[int] = []
                    for i in range(attribute.attr.__len__()):
                        i64_array.append(attribute.attr.__getitem__(i))
                    graph_op["attrs"][attribute.name] = {
                        "type": "s64[]",
                        "value": i64_array,
                    }
                elif isinstance(attribute.attr, DenseF32ArrayAttr):
                    f32_array: List[float] = []
                    for i in range(attribute.attr.__len__()):
                        f32_array.append(attribute.attr.__getitem__(i))
                    graph_op["attrs"][attribute.name] = {
                        "type": "f32[]",
                        "value": f32_array,
                    }
                elif isinstance(attribute.attr, StringAttr):
                    graph_op["attrs"][attribute.name] = {
                        "type": "string",
                        "value": attribute.attr.__str__(),
                    }

            for operand in op.operands:
                if operand.get_name() not in self.var_to_id:
                    self.var_to_id[operand.get_name()] = len(self.var_to_id)
                graph_op["inputs"].append(
                    {
                        "id": self.var_to_id[operand.get_name()],
                        "dtype": str(operand.type.element_type),
                        "shape": operand.type.shape,
                        "stride": self.gen_stride(operand.type.shape),
                        "layout_type": "strided",
                        "property_type": "undef",
                    }
                )
            for result in op.results:
                if result.get_name() not in self.var_to_id:
                    self.var_to_id[result.get_name()] = len(self.var_to_id)
                graph_op["outputs"].append(
                    {
                        "id": self.var_to_id[result.get_name()],
                        "dtype": str(result.type.element_type),
                        "shape": result.type.shape,
                        "stride": self.gen_stride(result.type.shape),
                        "layout_type": "strided",
                        "property_type": "undef",
                    }
                )

            graph_op["id"] = len(self.graph_object["graph"])
            graph_op["name"] = dialect_call
            self.graph_object["graph"].append(graph_op)

        for region in op.regions:
            self.dfs_region(region)

    def dfs_region(self, region: gc_mlir.ir.Region):
        for block in region.blocks:
            self.dfs_block(block)

    def dfs_block(self, block: gc_mlir.ir.Block):
        for op in block.operations:
            self.dfs_op(op)
